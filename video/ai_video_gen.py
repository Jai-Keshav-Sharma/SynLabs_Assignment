# -*- coding: utf-8 -*-
"""AI_Video_gen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ELJjy4YAGdauBMeLTxYHjAMb1xRvWQzs
"""

!pip install -q \
  langgraph \
  feedparser \
  openai \
  moviepy \
  diffusers \
  transformers \
  accelerate \
  torch

import os
import feedparser
import textwrap
from typing import List, TypedDict

import torch
from diffusers import DiffusionPipeline

from moviepy.editor import (
    ImageClip,
    CompositeVideoClip,
    concatenate_videoclips,
    TextClip
)

from langgraph.graph import StateGraph
from openai import OpenAI

# Set your OpenAI key
os.environ["OPENAI_API_KEY"] = "sk-proj..."

client = OpenAI()

pipe = DiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16,
    use_safetensors=True,
    variant="fp16"
)

pipe.to("cuda")

from typing import TypedDict, List

class WorkflowState(TypedDict, total=False):
    news: dict
    context: str
    script: List[str]
    scenes: List[dict]
    image_prompts: List[str]
    images: List[str]
    video: str

def fetch_trending_news(state: WorkflowState):
    rss_url = (
        "https://news.google.com/rss/headlines/section/topic/TECHNOLOGY"
        "?hl=en-IN&gl=IN&ceid=IN:en"
    )

    feed = feedparser.parse(rss_url)
    entry = feed.entries[0]

    print(f"Raw Trending News: {entry.title}")
    print(f"Summary: {entry.summary}\n\n")

    return {
        **state,
        "news": {
            "title": entry.title,
            "summary": entry.summary,
            "link": entry.link
        }
    }

def clean_and_contextualize(state: WorkflowState):
    news = state["news"]

    prompt = f"""
    You are preparing background context for an AI-generated news video.

    Article Title:
    {news['title']}

    Article Summary:
    {news['summary']}

    Task:
    - Extract only the key facts
    - Remove clickbait
    - Write a concise factual context (max 120 words)
    """

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )

    print(f"Cleaned Topic: {response.choices[0].message.content.strip()}\n\n")

    return {
        **state,
        "context": response.choices[0].message.content.strip()
    }

def generate_script(state: WorkflowState):
    context = state["context"]

    prompt = f"""
    Using the context below, write a 30–60 second narration script.

    Rules:
    - 7 to 8 short sentences
    - EACH SENTENCE MUST END WITH A NEW LINE
    - No numbering
    - Each sentence = one visual scene
    - Neutral tone

    NOTE: GENERATE AT LEAST 7 Short sentences, each sentece will be attached to a frame, and i want 7 frames AT LEAST.

    Context:
    {context}
    """

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )

    script_lines = [
        line.strip("-•0123456789. ").strip()
        for line in response.choices[0].message.content.split("\n")
        if line.strip()
    ]

    print(f"Script: \n{script_lines}\n\n")

    return {
        **state,
        "script": script_lines
    }

def plan_scenes(state: WorkflowState):
    scenes = []

    for line in state["script"]:
        prompt = f"""
        Convert this narration line into a realistic news image description.

        Line:
        "{line}"

        Output:
        One concise visual description.
        """

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}]
        )

        scenes.append({
            "line": line,
            "visual": response.choices[0].message.content.strip()
        })

    print(f"Scenes: \n {scenes}\n\n")

    return {
        **state,
        "scenes": scenes
    }

def engineer_prompts(state: WorkflowState):
    prompts = []

    for scene in state["scenes"]:
        prompt = f"""
        Rewrite the following image description into a Stable Diffusion XL prompt
        that is AT MOST 70 tokens long.

        Rules:
        - Keep the main subject
        - Keep the environment
        - Keep the mood
        - Remove unnecessary adjectives
        - No text, no watermark
        - Photorealistic, cinematic, news photography style
        - AVOID USING HEAVY TEXT INSIDE IMAGES BECAUSE DIFFUSION MODELS ARE BAD AT GENERATING TEXT INSIDE IMAGES.

        Description:
        {scene['visual']}
        """

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}]
        )

        compressed_prompt = response.choices[0].message.content.strip()
        prompts.append(compressed_prompt)

    print(f"Prompts: \n {prompts}\n\n")

    return {
        **state,
        "image_prompts": prompts
    }

def generate_images(state: WorkflowState):
    image_paths = []

    for i, prompt in enumerate(state["image_prompts"]):
        image = pipe(prompt=prompt).images[0]
        path = f"scene_{i}.png"
        image.save(path)
        image_paths.append(path)

    return {
        **state,
        "images": image_paths
    }

from PIL import Image as PILImage, ImageDraw, ImageFont
import textwrap

def draw_text_on_image(image_path, text, output_path):
    img = PILImage.open(image_path).convert("RGBA")

    img_width, img_height = img.size

    # ===== TARGET DIMENSIONS =====
    target_width = int(img_width * 0.90)
    target_height = int(img_height * 0.30)

    # ===== BIGGER FONT SIZE =====
    font_size = int(img_height * 0.12)  # 12% of image height

    try:
        font = ImageFont.truetype(
            "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf",
            font_size
        )
    except:
        font = ImageFont.load_default()
        font_size = 20  # fallback size

    # ===== TEXT WRAPPING =====
    # Create temporary draw object for measurements
    temp_draw = ImageDraw.Draw(img)

    # Estimate characters per line (conservative estimate)
    sample_width = temp_draw.textbbox((0, 0), "M" * 10, font=font)[2]
    chars_per_line = max(10, int((target_width * 0.9) / (sample_width / 10)))

    wrapped_text = textwrap.fill(text, width=chars_per_line)

    # ===== BACKGROUND BOX =====
    overlay = PILImage.new("RGBA", img.size, (0, 0, 0, 0))
    overlay_draw = ImageDraw.Draw(overlay)

    box_x1 = (img_width - target_width) // 2
    box_y1 = img_height - target_height
    box_x2 = box_x1 + target_width
    box_y2 = img_height

    overlay_draw.rectangle(
        [(box_x1, box_y1), (box_x2, box_y2)],
        fill=(0, 0, 0, 140)
    )

    img = PILImage.alpha_composite(img, overlay)
    draw = ImageDraw.Draw(img)

    # ===== MEASURE AND POSITION TEXT =====
    text_bbox = draw.multiline_textbbox(
        (0, 0),
        wrapped_text,
        font=font,
        spacing=15
    )
    text_width = text_bbox[2] - text_bbox[0]
    text_height = text_bbox[3] - text_bbox[1]

    x = (img_width - text_width) // 2
    y = img_height - target_height + (target_height - text_height) // 2

    # ===== DRAW TEXT =====
    stroke_width = max(3, font_size // 20)

    draw.multiline_text(
        (x, y),
        wrapped_text,
        font=font,
        fill="white",
        align="center",
        spacing=15,
        stroke_width=stroke_width,
        stroke_fill="black"
    )

    img.convert("RGB").save(output_path)





def compose_video(state: WorkflowState):
    clips = []
    duration = 5
    processed_images = []

    for i, (img_path, text) in enumerate(zip(state["images"], state["script"])):
        out_path = f"frame_{i}.png"
        draw_text_on_image(img_path, text, out_path)
        processed_images.append(out_path)

    for img in processed_images:
        clip = ImageClip(img).set_duration(duration)
        clips.append(clip)

    video = concatenate_videoclips(clips)
    video.write_videofile("final_video.mp4", fps=24)

    return {
        **state,
        "video": "final_video.mp4"
    }

graph = StateGraph(WorkflowState)

graph.add_node("fetch", fetch_trending_news)
graph.add_node("clean", clean_and_contextualize)
graph.add_node("script", generate_script)
graph.add_node("scenes", plan_scenes)
graph.add_node("prompts", engineer_prompts)
graph.add_node("images", generate_images)
graph.add_node("video", compose_video)

graph.set_entry_point("fetch")

graph.add_edge("fetch", "clean")
graph.add_edge("clean", "script")
graph.add_edge("script", "scenes")
graph.add_edge("scenes", "prompts")
graph.add_edge("prompts", "images")
graph.add_edge("images", "video")

app = graph.compile()

from IPython.display import Image, display
display(Image(app.get_graph().draw_mermaid_png()))

final_state = app.invoke({})
final_state["video"]
